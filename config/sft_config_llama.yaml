output_dir: "models/llama-8B-sft"
logging_steps: 50
per_device_train_batch_size: 8
report_to: wandb
gradient_accumulation_steps: 1
gradient_checkpointing: true
bf16: true
torch_empty_cache_steps: null
dataloader_pin_memory: true
dataloader_num_workers: 8
optim: "adamw_torch_fused"
torch_compile: false
num_train_epochs: 10
learning_rate: 0.0000005
save_strategy: "epoch"
logging_steps: 50
logging_dir: "logs"
run_name: "SFT-Llama31-8B"